{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56d4ed7d-6012-4830-bcac-9b89a4a415a3",
   "metadata": {},
   "source": [
    "# **Ingenier√≠a de Caracter√≠sticas - `05_ingenieria_caracteristicas.ipynb`**\n",
    "\n",
    "### **üéØ Objetivo del Notebook**\n",
    "Este notebook tiene como objetivo aplicar t√©cnicas de **ingenier√≠a de caracter√≠sticas** sobre las se√±ales EEG, con el fin de capturar mejor la informaci√≥n contenida en las se√±ales y mejorar el rendimiento de los modelos de clasificaci√≥n.\n",
    "\n",
    "### **üìå Contexto**\n",
    "En el notebook anterior (`04_analisis_caracteristicas.ipynb`), analizamos la importancia de las caracter√≠sticas existentes y aplicamos varias t√©cnicas de selecci√≥n y reducci√≥n de dimensionalidad. Sin embargo, concluimos que:\n",
    "\n",
    "- La **eliminaci√≥n de outliers** fue el √∫nico preprocesamiento que mejor√≥ el rendimiento.\n",
    "- La **selecci√≥n de caracter√≠sticas** y **reducci√≥n de dimensionalidad** no s√≥lo no aportaron mejoras, sino que en muchos casos **empeoraron los resultados**.\n",
    "\n",
    "Dado que los datos EEG son se√±ales **temporales complejas**, y que el modelo actual no logra capturar suficientemente su estructura din√°mica con las caracter√≠sticas crudas, es necesario **generar nuevas variables** que representen mejor la informaci√≥n relevante en la se√±al.\n",
    "\n",
    "---\n",
    "\n",
    "### **üß† ¬øPor qu√© Ingenier√≠a de Caracter√≠sticas?**\n",
    "\n",
    "Incluso los mejores modelos no pueden rendir bien si no se les alimenta con datos representativos. Por eso, vamos a construir nuevas variables derivadas de las se√±ales originales que puedan captar:\n",
    "\n",
    "- Tendencias locales (media m√≥vil)\n",
    "- Cambios r√°pidos (derivadas o gradientes)\n",
    "- Patrones en el dominio de la frecuencia (FFT)\n",
    "- Niveles de variabilidad o actividad (desviaci√≥n est√°ndar, rango, energ√≠a)\n",
    "\n",
    "Estas transformaciones buscan **extraer informaci√≥n latente** que no es evidente en las se√±ales brutas.\n",
    "\n",
    "---\n",
    "\n",
    "### **üöÄ Flujo de Trabajo en este Notebook**\n",
    "1Ô∏è‚É£ **Carga de datos**    \n",
    "2Ô∏è‚É£ **Definici√≥n de ventanas temporales para extracci√≥n de caracter√≠sticas**    \n",
    "3Ô∏è‚É£ **C√°lculo de estad√≠sticas temporales (media, std, varianza, gradiente)**  \n",
    "4Ô∏è‚É£ **Aplicaci√≥n de transformadas en frecuencia (FFT, potencia espectral)**  \n",
    "5Ô∏è‚É£ **Generaci√≥n de nuevas variables y construcci√≥n de un nuevo conjunto de datos**  \n",
    "6Ô∏è‚É£ **Evaluaci√≥n del impacto en el rendimiento del modelo**  \n",
    "7Ô∏è‚É£ **Conclusi√≥n sobre qu√© variables se mantienen y pr√≥ximos pasos**\n",
    "\n",
    "*Cargamos los datos sin preprocesar y eliminamos `outliers` pero todav√≠a no normalizamos ya que las transformaciones que haremos, como calcular medias, desviaciones, FFT o energ√≠a, dependen de la forma y magnitud original de la se√±al. Si normaliz√°ramos antes, perder√≠amos esa informaci√≥n significativa. Por eso, aplicaremos la normalizaci√≥n despu√©s de extraer las nuevas caracter√≠sticas.\n",
    "\n",
    "---\n",
    "\n",
    "üìå Si estas nuevas variables aportan mejoras, las incorporaremos como parte del preprocesamiento final antes de probar modelos m√°s avanzados como **XGBoost, LightGBM o Redes Neuronales**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c524f2-2568-4e9f-b433-dc85af06a191",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c6efad-76da-4586-8397-6c155551a57a",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a72fed2-d74f-47ef-be07-f2dff73fdf35",
   "metadata": {},
   "source": [
    "## **1. Carga de datos y Eliminaci√≥n Outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdb0df3c-1b59-41bd-a169-681246e8b92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_filtered shape: (1043205, 32)\n",
      "y_train_filtered shape: (1043205, 6)\n",
      "X_valid shape: (236894, 32)\n",
      "y_valid shape: (236894, 6)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "\n",
    "DATA_PATH = r\"C:\\Users\\luciaft\\Documents\\TFG\\TFG\\graspAndLiftDetectionTFGProyect\\data\\raw_data\\train\\train\"\n",
    "SUBJECT = \"subj1\"\n",
    "SERIES_TRAIN = [f\"{SUBJECT}_series{i}_data.csv\" for i in range(1, 9)]\n",
    "SERIES_EVENTS = [f\"{SUBJECT}_series{i}_events.csv\" for i in range(1, 9)]\n",
    "\n",
    "def load_data(series, path=DATA_PATH):\n",
    "    dfs = [pd.read_csv(os.path.join(path, file)) for file in series]\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df_train = load_data(SERIES_TRAIN)\n",
    "df_events = load_data(SERIES_EVENTS)\n",
    "df = df_train.merge(df_events, on=\"id\")\n",
    "\n",
    "eeg_columns = df.columns[1:33]  # Columnas de se√±ales EEG\n",
    "event_columns = [\"HandStart\", \"FirstDigitTouch\", \"BothStartLoadPhase\", \"LiftOff\", \"Replace\", \"BothReleased\"]\n",
    "\n",
    "df_sujeto1 = df[df[\"id\"].str.startswith(\"subj1_series\")]\n",
    "df_train = df_sujeto1[df_sujeto1[\"id\"].str.contains(\"series[1-6]_\")]\n",
    "df_valid = df_sujeto1[df_sujeto1[\"id\"].str.contains(r\"series[7-8]_\", regex=True)]\n",
    "\n",
    "X_train, y_train = df_train[eeg_columns], df_train[event_columns]\n",
    "X_valid, y_valid = df_valid[eeg_columns], df_valid[event_columns]\n",
    "\n",
    "# Eliminaci√≥n de Outliers\n",
    "z_scores = np.abs(zscore(X_train))\n",
    "outlier_threshold = 3\n",
    "mask = (z_scores < outlier_threshold).all(axis=1)\n",
    "X_train_filtered = X_train[mask]\n",
    "y_train_filtered = y_train[mask]\n",
    "\n",
    "print(f\"X_train_filtered shape: {X_train_filtered.shape}\")\n",
    "print(f\"y_train_filtered shape: {y_train_filtered.shape}\")\n",
    "print(f\"X_valid shape: {X_valid.shape}\")\n",
    "print(f\"y_valid shape: {y_valid.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4669c5da-50f4-485a-b07c-1e0cce7e3c7d",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4923cb74-22b7-4d0e-84de-1735d0f17544",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b07c008-4163-4836-8bb5-7745968302cf",
   "metadata": {},
   "source": [
    "## **2. Creaci√≥n Ventanas Temporales**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a93ca4-1726-448a-80c4-969db7cf2a55",
   "metadata": {},
   "source": [
    "Las ventanas sirven para dividir la se√±al EEG continua en segmentos cortos que nos permiten extraer estad√≠sticas (como media, energ√≠a o frecuencia) en intervalos de tiempo. As√≠ captamos c√≥mo cambia la actividad cerebral antes, durante y despu√©s de un evento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "201312df-a95a-4158-b8ec-c0b98080e075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ventanas creadas:\n",
      "X_train_win shape: (1043205, 128, 32)\n",
      "y_train_win shape: (1043205, 6)\n"
     ]
    }
   ],
   "source": [
    "# Par√°metros\n",
    "window_size = 128  # duraci√≥n de la ventana\n",
    "step_size = 1      # paso entre ventanas de 1 para tener una predicci√≥n por fila\n",
    "\n",
    "def create_windows_with_padding(X, y, window_size, step_size):\n",
    "    X_windows, y_windows = [], []\n",
    "    \n",
    "    # Padding con ceros al principio\n",
    "    pad = pd.DataFrame(0, index=range(window_size - 1), columns=X.columns)\n",
    "    X_padded = pd.concat([pad, X], ignore_index=True)\n",
    "    \n",
    "    for i in range(0, X.shape[0], step_size):\n",
    "        window = X_padded.iloc[i:i+window_size]\n",
    "        label = y.iloc[i]  # se mantiene la etiqueta real correspondiente a la fila original\n",
    "        X_windows.append(window)\n",
    "        y_windows.append(label)\n",
    "        \n",
    "    return np.array(X_windows, dtype=np.float32), pd.DataFrame(y_windows, columns=y.columns)\n",
    "\n",
    "\n",
    "# Aplicar a los datos\n",
    "X_train_win, y_train_win = create_windows_with_padding(X_train_filtered, y_train_filtered, window_size, step_size)\n",
    "X_valid_win, y_valid_win = create_windows_with_padding(X_valid, y_valid, window_size, step_size)\n",
    "\n",
    "# Verificar dimensiones\n",
    "print(\"Ventanas creadas:\")\n",
    "print(f\"X_train_win shape: {X_train_win.shape}\")\n",
    "print(f\"y_train_win shape: {y_train_win.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c98054-416e-4b47-b988-415e9d55fb4a",
   "metadata": {},
   "source": [
    "### Justificaci√≥n de la selecci√≥n de `step = 1` y `window_size = 120`\n",
    "\n",
    "Para la fase de ingenier√≠a de caracter√≠sticas, se debe definir correctamente la forma en que se crean las ventanas temporales sobre las se√±ales EEG. Esta decisi√≥n afecta directamente al rendimiento del modelo y debe basarse en el comportamiento real de los datos.\n",
    "\n",
    "#### Frecuencia de muestreo\n",
    "\n",
    "Seg√∫n la documentaci√≥n oficial del conjunto de datos, las se√±ales EEG han sido registradas con una frecuencia de muestreo de 500 Hz. Esto significa que cada fila del dataset representa 2 milisegundos de se√±al.\n",
    "\n",
    "#### Observaci√≥n directa de los eventos\n",
    "\n",
    "Se analizaron manualmente las etiquetas de los eventos en uno de los primeros intentos del sujeto 1. Se observ√≥ que los eventos tienen duraciones t√≠picas de entre 50 y 150 muestras, lo que equivale a intervalos de aproximadamente 100 a 300 milisegundos. Adem√°s, los eventos no siempre son exclusivos y pueden solaparse parcialmente.\n",
    "\n",
    "Por ejemplo:\n",
    "- El evento `HandStart` abarca unas 147 muestras (~294 ms).\n",
    "- El evento `LiftOff` abarca unas 77 muestras (~154 ms).\n",
    "- El evento `BothReleased` abarca unas 120 muestras (~240 ms).\n",
    "\n",
    "Estos valores indican que los eventos no son instant√°neos, sino procesos que se desarrollan a lo largo de decenas o centenas de muestras.\n",
    "\n",
    "#### Elecci√≥n de `window_size = 120`\n",
    "\n",
    "Se selecciona un tama√±o de ventana de 120 muestras (~240 ms) porque permite capturar con precisi√≥n la din√°mica completa de la mayor√≠a de los eventos observados, sin que la ventana se quede demasiado corta o abarque demasiado contexto irrelevante.\n",
    "\n",
    "#### Elecci√≥n de `step = 1`\n",
    "\n",
    "Dado que cada fila corresponde a 2 ms, usar `step = 1` garantiza que se genere una predicci√≥n para cada instante temporal, como exige la competencia. De esta forma, se preserva la resoluci√≥n original de los datos y se evitan saltos en las predicciones.\n",
    "\n",
    "#### Consideraciones adicionales\n",
    "\n",
    "Ventanas m√°s peque√±as podr√≠an no capturar toda la informaci√≥n relevante de un evento. Por el contrario, ventanas mucho m√°s grandes podr√≠an introducir ruido de eventos anteriores o futuros, lo que afectar√≠a la precisi√≥n del modelo.\n",
    "\n",
    "En cualquier caso, se probar√°n diferentes tama√±os de ventana (`window_size`) durante la experimentaci√≥n para comprobar si se puede mejorar el rendimiento del modelo.\n",
    "\n",
    "## Justificaci√≥n del uso de ventanas causales y padding inicial\n",
    "\n",
    "En sistemas de predicci√≥n en tiempo real, como las interfaces cerebro-computadora (BCI) o pr√≥tesis inteligentes, es fundamental que los modelos operen **de forma causal**, es decir, utilizando √∫nicamente informaci√≥n del pasado y del presente. Esta pr√°ctica garantiza que el sistema pueda desplegarse en entornos reales sin depender de datos futuros, lo cual ser√≠a imposible f√≠sicamente y conducir√≠a a errores graves de implementaci√≥n o evaluaci√≥n (*lookahead bias*).\n",
    "\n",
    "### Implementaci√≥n de ventanas causales\n",
    "\n",
    "Para alinear este trabajo con las buenas pr√°cticas en entornos reales, se ha implementado una **ventana deslizante causal**. En concreto:\n",
    "\n",
    "- Para predecir el estado en el instante `t`, se utiliza una ventana de tama√±o fijo que contiene los datos `[t - window_size, ..., t - 1]`.\n",
    "- En ning√∫n caso se accede a valores futuros, cumpliendo con los requisitos de sistemas en tiempo real.\n",
    "- La primera predicci√≥n v√°lida se realiza en `t = window_size`, ya que no existe suficiente historial antes de ese punto.\n",
    "\n",
    "Esta estrategia est√° ampliamente documentada en la literatura cient√≠fica y t√©cnica, donde se se√±ala que la **causalidad es un requisito indispensable** para que un sistema predictivo pueda operar en tiempo real (Simard, 2020; Lyons, 2011; Widrow & Stearns, 1985). El uso de ventanas causales tambi√©n evita la fuga de informaci√≥n futura, que podr√≠a inflar artificialmente el rendimiento del modelo durante la evaluaci√≥n.\n",
    "\n",
    "### Justificaci√≥n del padding con ceros al inicio\n",
    "\n",
    "En este trabajo se justifica tambi√©n el uso de **padding con ceros** en las primeras `window_size - 1` filas, donde el modelo a√∫n no dispone de suficiente historial para generar predicciones. Esta decisi√≥n se basa en los siguientes puntos:\n",
    "\n",
    "- En todas las series del conjunto de datos, las primeras ‚âà1000 filas no presentan ning√∫n evento (todas las etiquetas son `[0, 0, 0, 0, 0, 0]`). Este patr√≥n constante permite asumir que en los primeros instantes **no se producen eventos relevantes** para la detecci√≥n.\n",
    "- El padding con ceros **respeta la causalidad**, ya que no introduce informaci√≥n futura.\n",
    "- Permite evitar la p√©rdida de predicciones al inicio, asegurando que el modelo pueda operar de forma continua desde el primer instante.\n",
    "- Est√° alineado con otras aplicaciones en tiempo real que tambi√©n aplican padding inicial con ceros o medias hist√≥ricas cuando el historial a√∫n no es completo (Zhu et al., 2020; Oppenheim & Schafer, 2010).\n",
    "\n",
    "Este enfoque garantiza una entrada uniforme al modelo desde el inicio, mejora la cobertura temporal de las predicciones y se adapta a la distribuci√≥n observada en los datos. Adem√°s, es coherente con la ingenier√≠a de sistemas en producci√≥n, donde es habitual aplicar un periodo de *warm-up* o inicializaci√≥n con valores neutros para permitir que el sistema entre en r√©gimen estable sin comprometer su validez.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7806455-17e6-42f2-8336-0624350e7cf0",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ce9f29-8f43-42b5-9b77-eb066b2880b8",
   "metadata": {},
   "source": [
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9daecfa-08de-4e0d-aeb6-b11c52c5adfd",
   "metadata": {},
   "source": [
    "## **3. Extraer Estad√≠sticas Temporales**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a9c326-d0b9-4083-8342-414a7755c611",
   "metadata": {},
   "source": [
    "**Para cada ventana y cada canal EEG vamos a sacar:**\n",
    "- Media\n",
    "- Desviaci√≥n est√°ndar\n",
    "- M√≠nimo y m√°ximo\n",
    "- Rango (max - min)\n",
    "- Mediana\n",
    "- Percentiles 25 y 75\n",
    "- Gradiente medio\n",
    "- Asimetr√≠a (skewness)\n",
    "- Curtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f88591-3ae6-4e13-818a-5b8c4160a082",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "```\n",
    "def extract_time_features(X_windows):\n",
    "    features = []\n",
    "    for window in X_windows:\n",
    "        stats = []\n",
    "        for ch in window.T:  # recorremos canales\n",
    "            ch_series = pd.Series(ch)\n",
    "            stats.extend([\n",
    "                ch_series.mean(),\n",
    "                ch_series.std(),\n",
    "                ch_series.min(),\n",
    "                ch_series.max(),\n",
    "                ch_series.max() - ch_series.min(),  # rango\n",
    "                ch_series.median(),\n",
    "                ch_series.quantile(0.25),\n",
    "                ch_series.quantile(0.75),\n",
    "                np.gradient(ch).mean(),\n",
    "                ch_series.skew(),\n",
    "                ch_series.kurt()\n",
    "            ])\n",
    "        features.append(stats)\n",
    "    return np.array(features)\n",
    "\n",
    "X_train_feats = extract_time_features(X_train_win)\n",
    "X_valid_feats = extract_time_features(X_valid_win)\n",
    "\n",
    "print(\"Nuevas caracter√≠sticas extra√≠das:\")\n",
    "print(f\"X_train_feats shape: {X_train_feats.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb186311-3f81-45c6-9468-69de1ef82aba",
   "metadata": {},
   "source": [
    "### **Versi√≥n optimizada del extractor temporal:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d31d06-65fa-46be-945d-4a4c216573de",
   "metadata": {},
   "source": [
    "Dado que el proceso de extracci√≥n de caracter√≠sticas temporales tardaba demasiado, optimizaremos el c√≥digo para reducir significativamente el tiempo de c√≥mputo sin perder precisi√≥n en los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a1d253-b4a1-434d-b347-ac4772e14260",
   "metadata": {},
   "source": [
    "```\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Funci√≥n para extraer estad√≠sticas de una sola ventana\n",
    "def extract_features_from_window(window):\n",
    "    stats = []\n",
    "    for ch in window.T:\n",
    "        stats.extend([\n",
    "            np.mean(ch),\n",
    "            np.std(ch),\n",
    "            np.min(ch),\n",
    "            np.max(ch),\n",
    "            np.ptp(ch),  # rango\n",
    "            np.median(ch),\n",
    "            np.percentile(ch, 25),\n",
    "            np.percentile(ch, 75),\n",
    "            np.mean(np.gradient(ch)),\n",
    "            skew(ch),\n",
    "            kurtosis(ch)\n",
    "        ])\n",
    "    return stats\n",
    "\n",
    "# Extracci√≥n en paralelo con joblib\n",
    "n_jobs = 6  # Usa 6 n√∫cleos\n",
    "X_train_feats = Parallel(n_jobs=n_jobs)(delayed(extract_features_from_window)(w) for w in X_train_win)\n",
    "X_valid_feats = Parallel(n_jobs=n_jobs)(delayed(extract_features_from_window)(w) for w in X_valid_win)\n",
    "\n",
    "# Convertir a arrays de tipo float32 para ahorrar memoria\n",
    "X_train_feats = np.array(X_train_feats, dtype=np.float32)\n",
    "X_valid_feats = np.array(X_valid_feats, dtype=np.float32)\n",
    "\n",
    "# Verificar dimensiones\n",
    "print(\"Nuevas caracter√≠sticas extra√≠das:\")\n",
    "print(f\"X_train_feats shape: {X_train_feats.shape}\")\n",
    "print(f\"X_valid_feats shape: {X_valid_feats.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd39c7f-4c17-47c5-bac3-da77af30f633",
   "metadata": {},
   "source": [
    "### Optimizaci√≥n del tiempo de extracci√≥n de caracter√≠sticas\n",
    "\n",
    "Para mejorar la eficiencia del procesamiento de datos, se implement√≥ una versi√≥n paralelizada del extractor de caracter√≠sticas estad√≠sticas sobre las ventanas EEG.\n",
    "\n",
    "#### Justificaci√≥n\n",
    "\n",
    "La funci√≥n original recorr√≠a cada ventana secuencialmente, lo que resultaba en tiempos de espera de m√°s de 30 minutos cuando se usaba un `step_size = 60`. Al cambiar a `step_size = 1` para cumplir con el requisito de la competici√≥n (una predicci√≥n por fila), el n√∫mero de ventanas generadas supera el mill√≥n, lo que hizo inviable continuar con un procesamiento secuencial (ya que tardar√≠a aproximadamente 30 horas en ejecutarse).\n",
    "\n",
    "#### Implementaci√≥n de la mejora\n",
    "\n",
    "Inicialmente se intent√≥ paralelizar el proceso con la librer√≠a `multiprocessing`, dividiendo el trabajo en varios n√∫cleos usando `Pool.map()`. Sin embargo, al ejecutar este enfoque en Jupyter Notebook, no se produjo el uso real de los n√∫cleos, ni se mostr√≥ un aprovechamiento efectivo de la CPU.\n",
    "\n",
    "Por tanto, se opt√≥ finalmente por utilizar `joblib.Parallel`, pero segu√≠a tardando mucho ya que en Jupyter no se aplicaba bien el uso simult√°neo de los n√∫cleos, por lo que se cre√≥ el archivo `test_parallel.py` dentro del directorio `test` dentro del directorio del proyecto, demostrando que se ejecutaba r√°pido y que efectivamente el problema era tratar de aplicar esta estrategia dentro de Jupyter, por lo que finalmente procedemos ejecutando la extracci√≥n de caracter√≠sticas temporales en un script independiente fuera de Jupyter llamado `extraer_caracteristicas.py` dentro del directorio `scripts` dentro del directorio del proyecto, permitiendo:\n",
    "\n",
    "- Procesar las ventanas en paralelo de forma eficiente.\n",
    "- Verificar que todos los n√∫cleos disponibles son aprovechados correctamente (aunque como comentamos se realiz√≥ una prueba previa de carga).\n",
    "\n",
    "Para ello guardaremos los datos de entrada (`X_train_win`, `X_valid_win`) en disco en varias partes para que no haya problemas con la memoria y ejecutamos el script desde terminal, el cual tambi√©n guarda los archivos generados con las caracter√≠sticas extra√≠das. Posteriormente, estos archivos se cargan de nuevo en el notebook para continuar registrando aqu√≠ todos los pasos del pipeline.\n",
    "\n",
    "#### Beneficios\n",
    "\n",
    "- **Reducci√≥n dr√°stica del tiempo de ejecuci√≥n** (de m√°s de 30 horas a aproximadamente 3 horas).\n",
    "- **Mismos resultados** que el enfoque secuencial.\n",
    "- **Cumplimiento del `step_size = 1`** sin penalizaci√≥n computacional.\n",
    "- **Uso real de todos los n√∫cleos f√≠sicos**, al ejecutar el script fuera de Jupyter, lo cual demostra ser clave para el rendimiento.\n",
    "\n",
    "Esta estrategia permite continuar con un pipeline profesional, eficiente y escalable sin sacrificar precisi√≥n en las predicciones por muestra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cb035e2-9631-4b86-a9a0-d8331307bc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_win guardado por partes.\n",
      "X_valid_win guardado por partes.\n"
     ]
    }
   ],
   "source": [
    "path = r\"C:\\Users\\luciaft\\Documents\\TFG\\TFG\\graspAndLiftDetectionTFGProyect\\data\\processed\\ventanas\"\n",
    "\n",
    "# Par√°metro: tama√±o de cada parte\n",
    "part_size = 100_000\n",
    "\n",
    "# Guardar X_train_win por partes\n",
    "for i in range(0, len(X_train_win), part_size):\n",
    "    np.save(os.path.join(path, f\"X_train_win_part{i//part_size}.npy\"), X_train_win[i:i+part_size])\n",
    "print(\"X_train_win guardado por partes.\")\n",
    "\n",
    "# Guardar X_valid_win por partes\n",
    "for i in range(0, len(X_valid_win), part_size):\n",
    "    np.save(os.path.join(path, f\"X_valid_win_part{i//part_size}.npy\"), X_valid_win[i:i+part_size])\n",
    "print(\"X_valid_win guardado por partes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f191c02-a962-4081-b6c3-258c4b6fc3a4",
   "metadata": {},
   "source": [
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935e731f-a838-4f06-ba52-501e32b60a41",
   "metadata": {},
   "source": [
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a1a9e0-8d5d-4e18-978c-647f7e4763dc",
   "metadata": {},
   "source": [
    "## **4. Extraer Caracter√≠sticas en Frecuencia (FFT)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fd5303-fc84-4930-82e4-2e4c5e05b584",
   "metadata": {},
   "source": [
    "El cerebro se comunica en diferentes frecuencias (ondas delta, theta, alfa, beta...), la FFT ayuda a ver qu√© tan activas est√°n esas bandas durante cada ventana. Al trabajar con se√±ales EEG, es fundamental capturar informaci√≥n tanto en el dominio temporal como en el de frecuencia. Para ello, en lugar de usar una FFT directa, utilizamos el m√©todo de Welch (`scipy.signal.welch`), que estima la potencia espectral de forma m√°s estable y robusta frente al ruido. Esto nos permite calcular de forma m√°s fiable la energ√≠a presente en las bandas caracter√≠sticas del EEG (delta, theta, alfa y beta), generando variables m√°s representativas para los modelos de clasificaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ecce70-5267-4e25-8985-30b413c5be23",
   "metadata": {},
   "source": [
    "```\n",
    "from scipy.signal import welch\n",
    "\n",
    "def extract_freq_features(X_windows, fs=128):\n",
    "    freq_feats = []\n",
    "    for window in X_windows:\n",
    "        features = []\n",
    "        for ch in window.T:\n",
    "            freqs, psd = welch(ch, fs=fs, nperseg=128)\n",
    "            bands = {\n",
    "                'delta': (0.5, 4),\n",
    "                'theta': (4, 8),\n",
    "                'alpha': (8, 13),\n",
    "                'beta': (13, 30)\n",
    "            }\n",
    "            for low, high in bands.values():\n",
    "                band_power = np.sum(psd[(freqs >= low) & (freqs < high)])\n",
    "                features.append(band_power)\n",
    "            features.append(np.sum(psd))  # energ√≠a total\n",
    "        freq_feats.append(features)\n",
    "    return np.array(freq_feats)\n",
    "\n",
    "X_train_freq = extract_freq_features(X_train_win)\n",
    "X_valid_freq = extract_freq_features(X_valid_win)\n",
    "\n",
    "print(\"Caracter√≠sticas en frecuencia extra√≠das:\")\n",
    "print(f\"X_train_freq shape: {X_train_freq.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80399f3a-b559-40f9-a5d4-32494c5b444f",
   "metadata": {},
   "source": [
    "Al igual que en la extracci√≥n de caracter√≠sticas estad√≠sticas temporales, las caracter√≠sticas en frecuencia tambi√©n se obtienen de forma paralela dentro del script Python `extraer_caracteristicas.py` plasmado a continuaci√≥n:\n",
    "```\n",
    "import os\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.signal import welch\n",
    "\n",
    "# Rutas\n",
    "input_dir = r\"C:\\Users\\luciaft\\Documents\\TFG\\TFG\\graspAndLiftDetectionTFGProyect\\data\\processed\\ventanas\"\n",
    "output_dir = r\"C:\\Users\\luciaft\\Documents\\TFG\\TFG\\graspAndLiftDetectionTFGProyect\\data\\processed\\ventanas\\caract\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Funci√≥n para cargar partes ordenadas\n",
    "def load_ordered_parts(prefix):\n",
    "    part_files = sorted(\n",
    "        [f for f in os.listdir(input_dir) if f.startswith(prefix) and f.endswith(\".npy\")],\n",
    "        key=lambda x: int(x.split(\"part\")[1].split(\".\")[0])  # orden por √≠ndice\n",
    "    )\n",
    "    print(f\"Cargando partes de {prefix}:\", part_files)\n",
    "    parts = [np.load(os.path.join(input_dir, f)) for f in part_files]\n",
    "    return np.concatenate(parts, axis=0)\n",
    "\n",
    "# Cargar y reconstruir X_train_win y X_valid_win\n",
    "X_train_win = load_ordered_parts(\"X_train_win_part\")\n",
    "X_valid_win = load_ordered_parts(\"X_valid_win_part\")\n",
    "\n",
    "# ========================\n",
    "# FUNCIONES DE CARACTER√çSTICAS\n",
    "# ========================\n",
    "\n",
    "# Funci√≥n para estad√≠sticas temporales\n",
    "def extract_features_from_window(window):\n",
    "    stats = []\n",
    "    for ch in window.T:\n",
    "        stats.extend([\n",
    "            np.mean(ch), np.std(ch), np.min(ch), np.max(ch), np.ptp(ch),\n",
    "            np.median(ch), np.percentile(ch, 25), np.percentile(ch, 75),\n",
    "            np.mean(np.gradient(ch)), skew(ch), kurtosis(ch)\n",
    "        ])\n",
    "    return stats\n",
    "\n",
    "def extract_time_features_parallel(X_windows, n_jobs=-1):\n",
    "    return np.array(\n",
    "        Parallel(n_jobs=n_jobs)(\n",
    "            delayed(extract_features_from_window)(win) for win in X_windows\n",
    "        ),\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "# Funci√≥n para caracter√≠sticas en frecuencia\n",
    "def extract_freq_features_from_window(window, fs=128):\n",
    "    features = []\n",
    "    for ch in window.T:\n",
    "        freqs, psd = welch(ch, fs=fs, nperseg=128)\n",
    "        for low, high in [(0.5, 4), (4, 8), (8, 13), (13, 30)]:\n",
    "            features.append(np.sum(psd[(freqs >= low) & (freqs < high)]))\n",
    "        features.append(np.sum(psd))  # energ√≠a total\n",
    "    return features\n",
    "\n",
    "def extract_freq_features_parallel(X_windows, fs=128, n_jobs=-1):\n",
    "    return np.array(\n",
    "        Parallel(n_jobs=n_jobs)(\n",
    "            delayed(extract_freq_features_from_window)(win, fs) for win in X_windows\n",
    "        ),\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "# ========================\n",
    "# EJECUCI√ìN\n",
    "# ========================\n",
    "\n",
    "print(\"Extrayendo caracter√≠sticas TEMPORALES para X_train...\")\n",
    "X_train_feats = extract_time_features_parallel(X_train_win)\n",
    "np.save(os.path.join(output_dir, \"X_train_feats.npy\"), X_train_feats)\n",
    "\n",
    "print(\"Extrayendo caracter√≠sticas TEMPORALES para X_valid...\")\n",
    "X_valid_feats = extract_time_features_parallel(X_valid_win)\n",
    "np.save(os.path.join(output_dir, \"X_valid_feats.npy\"), X_valid_feats)\n",
    "\n",
    "print(\"Extrayendo caracter√≠sticas en FRECUENCIA para X_train...\")\n",
    "X_train_freq = extract_freq_features_parallel(X_train_win)\n",
    "np.save(os.path.join(output_dir, \"X_train_freq.npy\"), X_train_freq)\n",
    "\n",
    "print(\"Extrayendo caracter√≠sticas en FRECUENCIA para X_valid...\")\n",
    "X_valid_freq = extract_freq_features_parallel(X_valid_win)\n",
    "np.save(os.path.join(output_dir, \"X_valid_freq.npy\"), X_valid_freq)\n",
    "\n",
    "print(\"‚úÖ Extracci√≥n completada y archivos guardados.\")\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Debido al gran tama√±o de los datos, tal y como mencionamos anteriormente los resultados se guardan divididos en varias partes (por ejemplo, `X_train_feats_part0.npy`, `X_train_feats_part1.npy`, etc.). Posteriormente, en este notebook, se cargan todas esas partes y se concatenan para reconstruir los arrays completos (`X_train_feats`, `X_train_freq`, etc.).\n",
    "\n",
    "Este procedimiento permite trabajar con los datos completos sin superar los l√≠mites de memoria del sistema, manteniendo la trazabilidad y eficiencia del procesamiento. Una vez concatenados, los arrays est√°n listos para continuar con el modelado y an√°lisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8854f6f2-d001-4466-85ba-8f898f9d1879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caracter√≠sticas cargadas:\n",
      "X_train_feats: (1043205, 352)\n",
      "X_valid_feats: (236894, 352)\n",
      "X_train_freq:  (1043205, 160)\n",
      "X_valid_freq:  (236894, 160)\n"
     ]
    }
   ],
   "source": [
    "# Ruta donde se guardaron las caracter√≠sticas\n",
    "caract_dir = r\"C:\\Users\\luciaft\\Documents\\TFG\\TFG\\graspAndLiftDetectionTFGProyect\\data\\processed\\ventanas\\caract\"\n",
    "\n",
    "# Cargar caracter√≠sticas temporales\n",
    "X_train_feats = np.load(os.path.join(caract_dir, \"X_train_feats.npy\"))\n",
    "X_valid_feats = np.load(os.path.join(caract_dir, \"X_valid_feats.npy\"))\n",
    "\n",
    "# Cargar caracter√≠sticas en frecuencia\n",
    "X_train_freq = np.load(os.path.join(caract_dir, \"X_train_freq.npy\"))\n",
    "X_valid_freq = np.load(os.path.join(caract_dir, \"X_valid_freq.npy\"))\n",
    "\n",
    "# Verificar dimensiones\n",
    "print(\"Caracter√≠sticas cargadas:\")\n",
    "print(f\"X_train_feats: {X_train_feats.shape}\")\n",
    "print(f\"X_valid_feats: {X_valid_feats.shape}\")\n",
    "print(f\"X_train_freq:  {X_train_freq.shape}\")\n",
    "print(f\"X_valid_freq:  {X_valid_freq.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b293ca87-2295-49be-a0e3-237bb7260f6c",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6473552-dbbc-4019-adf2-9931ae26b4f1",
   "metadata": {},
   "source": [
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93740150-a81f-4648-975f-bfe631af1527",
   "metadata": {},
   "source": [
    "## **5. Generaci√≥n Nuevas Variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cce6fd-c28d-44f8-a170-f457f9afb0ab",
   "metadata": {},
   "source": [
    "Unimos los datos de las estad√≠sticas temporales y de las caracter√≠sticas en frecuencia, y normalizamos los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a252d3c6-ee50-4c31-896a-940f1225cdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = np.concatenate([X_train_feats, X_train_freq], axis=1)\n",
    "X_valid_final = np.concatenate([X_valid_feats, X_valid_freq], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "251737fb-7477-405f-b90a-f81c473d70a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_final)\n",
    "X_valid_scaled = scaler.transform(X_valid_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbe6938-3727-409a-9dd1-083c2e29db30",
   "metadata": {},
   "source": [
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a06032-1b18-4f67-9e5c-68a376bd7d0c",
   "metadata": {},
   "source": [
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4759a96-1a40-44fb-a73c-5398b9a2da47",
   "metadata": {},
   "source": [
    "## **6. Entrenar y Evaluar Modelos**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "905c17ba-1737-45b9-b512-a44c632828b9",
   "metadata": {},
   "source": [
    "Finalmente entrenamos con estas nuevas variables el modelo base que hasta ahora mejores resultados ha demostrado, **Regresi√≥n Log√≠stica**, y lo evaluamos para verificar si estas nuevas caracter√≠sticas realmente mejoran el rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c73696a3-31b0-4183-8031-92f6b9726caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluando evento: HandStart\n",
      "HandStart: AUC = 0.8928\n",
      "\n",
      "Evaluando evento: FirstDigitTouch\n",
      "FirstDigitTouch: AUC = 0.8838\n",
      "\n",
      "Evaluando evento: BothStartLoadPhase\n",
      "BothStartLoadPhase: AUC = 0.8888\n",
      "\n",
      "Evaluando evento: LiftOff\n",
      "LiftOff: AUC = 0.9059\n",
      "\n",
      "Evaluando evento: Replace\n",
      "Replace: AUC = 0.8966\n",
      "\n",
      "Evaluando evento: BothReleased\n",
      "BothReleased: AUC = 0.8751\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def evaluate_model(X_train, y_train, X_valid, y_valid, event_name):\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict_proba(X_valid)[:, 1]\n",
    "    auc = roc_auc_score(y_valid, y_pred)\n",
    "    print(f\"{event_name}: AUC = {auc:.4f}\")\n",
    "    return auc\n",
    "\n",
    "# Evaluaci√≥n por evento\n",
    "for i, event in enumerate(y_train_win.columns):\n",
    "    print(f\"\\nEvaluando evento: {event}\")\n",
    "    auc = evaluate_model(\n",
    "        X_train_scaled, y_train_win[event],\n",
    "        X_valid_scaled, y_valid_win[event],\n",
    "        event\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff99de7-7d87-4125-b2d2-91f72abeeefb",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ec437e-c636-46bd-a091-ad327ddcd1fa",
   "metadata": {},
   "source": [
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6122c4eb-3f6e-4197-a369-c89b6fe08b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Ruta de guardado\n",
    "processed_path = r\"C:\\Users\\luciaft\\Documents\\TFG\\TFG\\graspAndLiftDetectionTFGProyect\\data\\processed\"\n",
    "os.makedirs(processed_path, exist_ok=True)\n",
    "\n",
    "# Guardar datos preprocesados finales (w/o outliers + features temporales + frecuencia + escalado)\n",
    "with open(os.path.join(processed_path, \"preprocessed_features_temporal_freq.pkl\"), \"wb\") as f:\n",
    "    pickle.dump((X_train_scaled, y_train_win, X_valid_scaled, y_valid_win), f)\n",
    "\n",
    "# Guardar en CSV para visualizaci√≥n r√°pida si hace falta\n",
    "pd.DataFrame(X_train_scaled).to_csv(os.path.join(processed_path, \"X_train_feats.csv\"), index=False)\n",
    "pd.DataFrame(y_train_win).to_csv(os.path.join(processed_path, \"y_train_feats.csv\"), index=False)\n",
    "pd.DataFrame(X_valid_scaled).to_csv(os.path.join(processed_path, \"X_valid_feats.csv\"), index=False)\n",
    "pd.DataFrame(y_valid_win).to_csv(os.path.join(processed_path, \"y_valid_feats.csv\"), index=False)\n",
    "\n",
    "# Guardar resultados AUC del modelo actual\n",
    "auc_dict = {\n",
    "    \"HandStart\": 0.8928,\n",
    "    \"FirstDigitTouch\": 0.8838,\n",
    "    \"BothStartLoadPhase\": 0.8888,\n",
    "    \"LiftOff\": 0.9059,\n",
    "    \"Replace\": 0.8966,\n",
    "    \"BothReleased\": 0.8751\n",
    "}\n",
    "\n",
    "auc_df = pd.DataFrame.from_dict(auc_dict, orient='index', columns=['AUC'])\n",
    "auc_df.index.name = 'Evento'\n",
    "auc_df.to_csv(os.path.join(processed_path, \"auc_results_feats_logreg.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a74893-7e3b-446c-b8c7-4677c8aa1441",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f67bd6-ca5c-48e7-90b4-35cf472d8ab2",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec86558-9345-4073-893e-3ec42c534eb6",
   "metadata": {},
   "source": [
    "## üìå **Conclusi√≥n y Pr√≥ximos Pasos**\n",
    "### Comparaci√≥n de Rendimiento: Antes vs Despu√©s de Ingenier√≠a de Caracter√≠sticas\n",
    "\n",
    "Comprobamos si las nuevas variables generadas (estad√≠sticas temporales + frecuencia mediante potencia espectral) mejoran el rendimiento del modelo. A continuaci√≥n, se comparan los valores de AUC-ROC obtenidos con:\n",
    "\n",
    "- **Datos preprocesados √∫nicamente eliminando outliers y normalizando**\n",
    "- **Nuevas caracter√≠sticas extra√≠das y normalizadas**\n",
    "\n",
    "| Evento                | AUC (antes) | AUC (con nuevas caracter√≠sticas) |\n",
    "|-----------------------|-------------|----------------------------------|\n",
    "| HandStart             | 0.718       | 0.8928                           |\n",
    "| FirstDigitTouch       | 0.694       | 0.8838                           |\n",
    "| BothStartLoadPhase    | 0.6922      | 0.8888                           |\n",
    "| LiftOff               | 0.7462      | 0.9059                           |\n",
    "| Replace               | 0.8501      | 0.8966                           |\n",
    "| BothReleased          | 0.808       | 0.8751                           |\n",
    "\n",
    "üü¢ **Conclusi√≥n**: Las nuevas caracter√≠sticas extra√≠das mejoran de forma clara y consistente el rendimiento del modelo en todos los eventos. Se observa un aumento especialmente significativo en eventos como *HandStart* y *LiftOff*, donde se alcanzan valores cercanos o superiores a 0.90 cuando antes no superaban 0.75 en AUC-ROC. Por tanto, estas nuevas variables se incorporar√°n al pipeline final de modelado. Tras mostrarse los resultados, se han guardado los datos preprocesados para ser utilizados en el siguiente notebook con modelos avanzados.\n",
    "\n",
    "### ‚úÖ Preprocesado Completo\n",
    "\n",
    "El conjunto de datos final preparado para entrenar modelos incluye los siguientes pasos:\n",
    "\n",
    "- **Filtrado de outliers**: Se eliminaron muestras extremas del conjunto de entrenamiento utilizando un umbral de z-score con valor absoluto superior a 3. Esta t√©cnica es habitual para eliminar artefactos sin eliminar datos √∫tiles. El conjunto de validaci√≥n se mantuvo sin alteraciones para evitar fugas de informaci√≥n.\n",
    "\n",
    "- **Ventaneo de las se√±ales**: Las se√±ales EEG se dividieron en ventanas deslizantes de 120 muestras (equivalente a 240‚ÄØms, con muestreo de 500‚ÄØHz), con un `step` de 1 muestra. Esto garantiza una predicci√≥n por cada instante temporal, como exige la competici√≥n. Adem√°s, cada ventana contiene √∫nicamente muestras **anteriores al instante actual**, siguiendo una l√≥gica causal compatible con sistemas en tiempo real. Para las primeras muestras sin historial suficiente, se aplic√≥ **padding con ceros**.\n",
    "\n",
    "- **Extracci√≥n de caracter√≠sticas**: Para cada ventana y canal se extrajeron:\n",
    "  - **Caracter√≠sticas temporales**: media, desviaci√≥n est√°ndar, m√≠nimo, m√°ximo, rango, mediana, percentiles 25 y 75, gradiente medio, asimetr√≠a (skewness) y curtosis.\n",
    "  - **Caracter√≠sticas en frecuencia**: energ√≠a espectral en las bandas delta (0.5‚Äì4‚ÄØHz), theta (4‚Äì8‚ÄØHz), alpha (8‚Äì13‚ÄØHz), beta (13‚Äì30‚ÄØHz), y energ√≠a total, a partir de la densidad espectral de potencia (PSD) calculada con el m√©todo de Welch.\n",
    "\n",
    "- **Normalizaci√≥n**: Las caracter√≠sticas extra√≠das fueron estandarizadas con `StandardScaler`, centrando en media 0 y varianza 1. Esto asegura que todas las variables tengan la misma escala y evita que algunas dominen sobre otras durante el entrenamiento.\n",
    "\n",
    "- **Alineaci√≥n temporal con las etiquetas**: Las etiquetas (`y_train_win`) se tomaron de la fila actual (la √∫ltima dentro de cada ventana), garantizando que las predicciones se realicen solo con informaci√≥n disponible hasta ese instante. Esta estrategia es compatible con aplicaciones reales en tiempo real como sistemas BCI.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Pr√≥ximos pasos - Notebook **`06_modelado_avanzado`**\n",
    "\n",
    "En el siguiente notebook entrenaremos modelos m√°s potentes para mejorar el rendimiento,  como:\n",
    "   - `RandomForestClassifier`\n",
    "   - `XGBoost`\n",
    "   - `LightGBM`\n",
    "   - Redes neuronales con `Keras`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
